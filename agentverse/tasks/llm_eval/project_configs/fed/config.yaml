task:
  llmeval
data_path:
  ./agentverse/tasks/llm_eval/data/fed/fed_data.json
output_dir:
  ./outputs/fed
prompts:
  prompt: &prompt |-
    [Dialog Context]
    ${source_text}

    [The Start of Assistant's Answer]
    ${response_to_evaluate}
    [The End of Assistant's Answer]
    
    [System]
    We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.
    Please consider the helpfulness, relevance, accuracy, and level of detail of their responses.
    There are a few other referee assigned the same task, it's your responsibility to discuss with them and think critically before you make your final judgement.
    Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.

    ${role_description}

    Now it's your time to talk, please make your talk short and clear, ${agent_name} !

    ${final_prompt}


environment:
  env_type: llm_eval
  max_turns: 5  # Se Ã¨ 4, l'ultimo Scientist non viene fatto
  rule:
    order:
      type: sequential
    visibility:
      type: all
    selector:
      type: basic
    updater:
      type: basic
    describer:
      type: basic

agents:
  -
    agent_type: llm_eval_multi
    name: General Public
    final_prompt_to_use: &final_prompt_definition |-
      Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias.
      Then, output one line indicating the overall score for the assistant on a scale of 1 to 10.

      Remember that you are not required to output the same value as other referees!
      Output with the following format strictly:
      Evaluation evidence: [your explanation here]
      Overall Score: [score only]
    role_description: |-
      You are now General Public, one of the referees in this task. You are interested in the story and looking for updates on the investigation. Please think critically by yourself and note that it's your responsibility to choose one of which is the better first.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-4.1-mini"
      llm_type: gpt-4.1-mini
      temperature: 0
      max_tokens: 512

  -
    agent_type: llm_eval_multi
    name: Critic
    final_prompt_to_use: *final_prompt_definition      # Stesso prompt definito sopra
    role_description: |-
      You are now Critic, one of the referees in this task. You will check fluent writing, clear sentences, and good wording in summary writing. Your job is to question others judgement to make sure their judgement is well-considered and offer an alternative solution if two responses are at the same level.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-4.1-mini"
      llm_type: gpt-4.1-mini
      temperature: 0
      max_tokens: 512
  
  -
    agent_type: llm_eval_multi
    name: News Author
    final_prompt_to_use: *final_prompt_definition      # Stesso prompt definito sopra
    role_description: |-
      You are News Author, one of the referees in this task. You will focus on the consistency with the original article. Please help other people to determine which response is the better one.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-4.1-mini"
      llm_type: gpt-4.1-mini
      temperature: 0
      max_tokens: 512
  

  -
    agent_type: llm_eval_multi
    name: Psychologist
    final_prompt_to_use: *final_prompt_definition      # Stesso prompt definito sopra
    role_description: |-
      You are Psychologist, one of the referees in this task. You will study human behavior and mental processes in order to understand and explain human behavior. Please help other people to determine which response is the better one.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-4.1-mini"
      llm_type: gpt-4.1-mini
      temperature: 0
      max_tokens: 512
  

  -
    agent_type: llm_eval_multi
    name: Scientist
    final_prompt_to_use: *final_prompt_definition      # Stesso prompt definito sopra
    role_description: |-
      You are Scientist, one of the referees in this task. You are a professional engaged in systematic study who possesses a strong background in the scientific method, critical thinking, and problem-solving abilities. Please help other people to determine which response is the better one.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-4.1-mini"
      llm_type: gpt-4.1-mini
      temperature: 0
      max_tokens: 512

tools: ~